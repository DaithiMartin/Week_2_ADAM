{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam: Exercise\n",
    "\n",
    "For this exercise we will be build Adam up from scratch starting with regular gradient descent. We will also be utilizing mini batches to introduce stochasticity to the optimization. \n",
    "\n",
    "We will be working with the mnist_784 data set and a simple shallow neural network. \n",
    "\n",
    "If you do not have scikit-learn then you can get it here: https://scikit-learn.org/stable/install.html\n",
    "\n",
    "This code is heavily inspired by Doug’s code from CSCI 447/547 lecture 05_multilayer_perceptron.\n",
    "\n",
    "First we need to get the data, define the network and define some functions to perform on the data. You don’t need to do anything with this first block of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X[::5]\n",
    "y = y.astype(int)[::5]\n",
    "X, X_test, y, y_test = train_test_split(X, y)\n",
    "\n",
    "# Here we specify the size of our neural network.\n",
    "# We are mapping from 784 to 10 with 256 hiden layer nodes.\n",
    "\n",
    "m = len(X)\n",
    "n_0 = 784\n",
    "n_1 = 256\n",
    "N = 10\n",
    "\n",
    "\n",
    "# Function to convert categorical labels into one-hot matrix.\n",
    "def convert_to_one_hot(y, n_classes):\n",
    "    T = np.zeros((y.shape[0], n_classes))\n",
    "    for t, yy in zip(T, y):\n",
    "        t[yy] = 1\n",
    "    return T\n",
    "\n",
    "\n",
    "# Convert the data to one hot notation\n",
    "one_hot_y_actual = convert_to_one_hot(y, N)\n",
    "one_hot_y_test = convert_to_one_hot(y_test, N)\n",
    "\n",
    "\n",
    "# Sigmoid function (activation)\n",
    "def sigmoid(a):\n",
    "    return 1. / (1 + np.exp(-a))\n",
    "\n",
    "\n",
    "# Softmax function (final layer for classification)\n",
    "def softmax(A):\n",
    "    numerator = np.exp(A)\n",
    "    denominator = numerator.sum(axis=1)\n",
    "    return numerator / denominator[:, np.newaxis]\n",
    "\n",
    "\n",
    "# Categorical cross-entropy\n",
    "def L(T, S, W1, W2, alpha_1=1e-2, alpha_2=1e-5):\n",
    "    return -1. / len(T) * np.sum(T * np.log(S)) + np.sum(0.5 * alpha_1 * W1 ** 2) + np.sum(0.5 * alpha_2 * W2 ** 2)\n",
    "\n",
    "\n",
    "# Run the neural network forward, given some weights and biases\n",
    "def feedforward(X, W1, W2, b1, b2):\n",
    "    # Feedforward\n",
    "    A1 = X @ W1 + b1\n",
    "    Z1 = sigmoid(A1)\n",
    "    A2 = Z1 @ W2 + b2\n",
    "    y_pred = softmax(A2)\n",
    "    return y_pred, Z1\n",
    "\n",
    "\n",
    "# Compute the neural network gradients using backpropagation\n",
    "def backpropogate(y_pred, Z1, X, y_obs, alpha_1=1e-2, alpha_2=1e-5):\n",
    "    # Backpropogate\n",
    "    delta_2 = (1. / len(y_pred)) * (y_pred - y_obs)\n",
    "    grad_W2 = Z1.T @ delta_2 + alpha_2 * W2\n",
    "    grad_b2 = delta_2.sum(axis=0)\n",
    "\n",
    "    delta_1 = delta_2 @ W2.T * Z1 * (1 - Z1)\n",
    "    grad_W1 = X.T @ delta_1 + alpha_1 * W1\n",
    "    grad_b1 = delta_1.sum(axis=0)\n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "\n",
    "\n",
    "def mini_batch(x_sample, y_sample, start_batch_size):\n",
    "    \"\"\"\n",
    "    Takes a copy of x_sample and y_sample and returns mini batch matrices of both and number of batches\n",
    "    \"\"\"\n",
    "\n",
    "    # Batches must divide evenly into total number of samples for numpy arrays to be happy.\n",
    "    # Gets number of bathes by finding next smallest number that evenly divides\n",
    "    num_batches = start_batch_size\n",
    "    while len(x_sample) % num_batches != 0:\n",
    "        num_batches -= 1\n",
    "\n",
    "    # randomly shuffle indices\n",
    "    np.random.seed(42)\n",
    "    random_indices = np.random.choice(range(len(x_sample)), len(x_sample), replace=False)\n",
    "\n",
    "    # instantiate lists to hold batches\n",
    "    x_list = [[] for i in range(num_batches)]\n",
    "    y_list = [[] for i in range(num_batches)]\n",
    "\n",
    "    # populate batches matrix with random mini batch indices\n",
    "    for i in range(len(x_sample)):\n",
    "\n",
    "        x_list[i // 105].append(x_sample[random_indices[i]])\n",
    "        y_list[i // 105].append(y_sample[random_indices[i]])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    x_batch = np.array(x_list)\n",
    "    y_batch = np.array(y_list)\n",
    "\n",
    "    return x_batch, y_batch, num_batches, num_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Gradient Descent\n",
    "\n",
    "For our first exercise we will implement plain old gradient descent. The mathematical formula is:\n",
    "\n",
    "$$ \\theta_t = \\theta_{t-1} - \\alpha \\nabla f(\\theta_{t-1}) \\tag{1}$$\n",
    "---\n",
    "\n",
    "We have already specified initial values for the $\\alpha$ and the batch size but feel free to play around with it. The location to insert your gradient descent implementation is outlined with a multi-line comment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vanilla Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-5\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "vanilla_loss = []\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "        \n",
    "        \"\"\"\n",
    "        These are your gradients with respect to weight matrices W1 and W2 \n",
    "        as well as your biases b1 and b2\n",
    "        \"\"\"\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "\n",
    "        \"\"\"\n",
    "        You put your code here. W1 and W2 are your weight matrices.\n",
    "        b1 and b2 are your bias for each matrix.\n",
    "        Each are numpy arrays.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    vanilla_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "plt.plot(vanilla_loss, label='Vanilla')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum Gradient Descent\n",
    "\n",
    "Next we will add momentum. The the new update rule is:\n",
    "\n",
    "$$m_{t}=\\beta_{1} m_{t-1} + (1 - \\beta_{1})\\nabla f(\\theta_{t-1})$$\n",
    "---\n",
    "\n",
    "$$ \\beta_1 \\in [0,1)$$\n",
    "---\n",
    "\n",
    "$$\\theta_{t}=\\theta_{t-1} - \\alpha m_{t} \\tag{2}$$\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Momentum Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-5\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\"\"\"\n",
    "You need to add another hyper parameter here. Hint! look at the equation above\n",
    "\"\"\"\n",
    "\n",
    "# data for analysis\n",
    "momentum_loss = []\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "\"\"\"\n",
    "You probably need to initialize your momentum here... \n",
    "\"\"\"\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "\n",
    "        \"\"\"\n",
    "        These are your gradients with respect to weight matrices W1 and W2 \n",
    "        as well as your biases b1 and b2\n",
    "        \"\"\"\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "\n",
    "        \"\"\"\n",
    "        You put your code here. W1 and W2 are your weight matrices.\n",
    "        b1 and b2 are your bias for each matrix.\n",
    "        Each are numpy arrays.\n",
    "        \"\"\"\n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    momentum_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "plt.plot(vanilla_loss, label='Vanilla')\n",
    "plt.plot(momentum_loss, label='Momentum')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp Gradient Descent \n",
    "\n",
    "Next we will do RMSProp. The update rule is as follows:\n",
    "\n",
    "$$v_{t} = \\beta_{2}v_{t-1} + (1 - \\beta_{2})\\nabla f(\\theta_{t-1})^2$$\n",
    "---\n",
    "\n",
    "$$ \\beta_2 \\in [0,1)$$\n",
    "---\n",
    "\n",
    "$$\\theta_{t} = \\theta_{t-1} - \\alpha \\frac { \\nabla f(\\theta_{t-1})}{\\sqrt{v_{t} + \\epsilon}} \\tag{3}$$ \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RMSProp Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-5\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\"\"\"\n",
    "You need to add two hyper parameters here. Hint! look at the equation above\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "RMS_loss = []\n",
    "\n",
    "\"\"\"\n",
    "You probably need to initialize your variance here...\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "\n",
    "        \"\"\"\n",
    "        These are your gradients with respect to weight matrices W1 and W2 \n",
    "        as well as your biases b1 and b2\n",
    "        \"\"\"\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "\n",
    "        \"\"\"\n",
    "        You put your code here. W1 and W2 are your weight matrices.\n",
    "        b1 and b2 are your bias for each matrix.\n",
    "        Each are numpy arrays.\n",
    "        \"\"\"\n",
    "\n",
    "       \n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    RMS_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "plt.plot(vanilla_loss, label='Vanilla')\n",
    "plt.plot(momentum_loss, label='Momentum')\n",
    "plt.plot(RMS_loss, label='RMSProp')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Gradient Descent \n",
    "\n",
    "Now we put them both together and we get Adam!\n",
    "\n",
    "$$m_{t}=\\beta_{1} m_{t-1} + (1 - \\beta_{1})\\nabla f(\\theta_{t-1})$$\n",
    "---\n",
    "\n",
    "$$v_{t} = \\beta_{2}v_{t-1} + (1 - \\beta_{2})\\nabla f(\\theta_{t-1})^2$$\n",
    "---\n",
    "\n",
    "$$\\hat m_{t} = \\frac {m_{t}}{1 - \\beta_1^{t}}$$\n",
    "---\n",
    "\n",
    "$$\\hat v_t = \\frac {v_{t}}{1 - \\beta_2^{t}}$$\n",
    "---\n",
    "\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\frac {\\hat m_t}{\\sqrt{\\hat v_t + \\epsilon}} \\tag{4}$$\n",
    "---\n",
    "\n",
    "Play around with the hyperparameters to see if you can get distinctly different behavior from the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adam Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-5\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\"\"\"\n",
    "You need to add 3 hyper parameters here. Hint! look at the equation above\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "Adam_loss = []\n",
    "\n",
    "\"\"\"\n",
    "You probably need to initialize your variance and momentum here...\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "\n",
    "        \"\"\"\n",
    "        These are your gradients with respect to weight matrices W1 and W2 \n",
    "        as well as your biases b1 and b2\n",
    "        \"\"\"\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "\n",
    "        \"\"\"\n",
    "        You put your code here. W1 and W2 are your weight matrices.\n",
    "        b1 and b2 are your bias for each matrix.\n",
    "        Each are numpy arrays.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    Adam_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "plt.plot(vanilla_loss, label='Vanilla')\n",
    "plt.plot(momentum_loss, label='Momentum')\n",
    "plt.plot(RMS_loss, label='RMSProp')\n",
    "plt.plot(Adam_loss, label='Adam')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
