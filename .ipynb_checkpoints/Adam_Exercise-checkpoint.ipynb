{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam: Exercise\n",
    "\n",
    "For this exercise we will be build Adam up from scratch starting with regurlar gradient descent. We will also be utilizing mini batches to introduce stochasticity to the optimization. \n",
    "\n",
    "For these exercises we will be working with the mnist_784 data set and a simple shallow neural network.  \n",
    "\n",
    "First we need to get the data and define the network. We have done this part for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X[::5]\n",
    "y = y.astype(int)[::5]\n",
    "X, X_test, y, y_test = train_test_split(X, y)\n",
    "\n",
    "# Here we specify the size of our neural network.\n",
    "# We are mapping from 784 to 10 with 256 hiden layer nodes.\n",
    "\n",
    "m = len(X)\n",
    "n_0 = 784\n",
    "n_1 = 256\n",
    "N = 10\n",
    "\n",
    "\n",
    "# Function to convert categorical labels into one-hot matrix.\n",
    "def convert_to_one_hot(y, n_classes):\n",
    "    T = np.zeros((y.shape[0], n_classes))\n",
    "    for t, yy in zip(T, y):\n",
    "        t[yy] = 1\n",
    "    return T\n",
    "\n",
    "\n",
    "# Convert the data to one hot notation\n",
    "one_hot_y_actual = convert_to_one_hot(y, N)\n",
    "one_hot_y_test = convert_to_one_hot(y_test, N)\n",
    "\n",
    "\n",
    "# Sigmoid function (activation)\n",
    "def sigmoid(a):\n",
    "    return 1. / (1 + np.exp(-a))\n",
    "\n",
    "\n",
    "# Softmax function (final layer for classification)\n",
    "def softmax(A):\n",
    "    numerator = np.exp(A)\n",
    "    denominator = numerator.sum(axis=1)\n",
    "    return numerator / denominator[:, np.newaxis]\n",
    "\n",
    "\n",
    "# Categorical cross-entropy\n",
    "def L(T, S, W1, W2, alpha_1=1e-2, alpha_2=1e-5):\n",
    "    return -1. / len(T) * np.sum(T * np.log(S)) + np.sum(0.5 * alpha_1 * W1 ** 2) + np.sum(0.5 * alpha_2 * W2 ** 2)\n",
    "\n",
    "\n",
    "# Run the neural network forward, given some weights and biases\n",
    "def feedforward(X, W1, W2, b1, b2):\n",
    "    # Feedforward\n",
    "    A1 = X @ W1 + b1\n",
    "    Z1 = sigmoid(A1)\n",
    "    A2 = Z1 @ W2 + b2\n",
    "    y_pred = softmax(A2)\n",
    "    return y_pred, Z1\n",
    "\n",
    "\n",
    "# Compute the neural network gradients using backpropagation\n",
    "def backpropogate(y_pred, Z1, X, y_obs, alpha_1=1e-2, alpha_2=1e-5):\n",
    "    # Backpropogate\n",
    "    delta_2 = (1. / len(y_pred)) * (y_pred - y_obs)\n",
    "    grad_W2 = Z1.T @ delta_2 + alpha_2 * W2\n",
    "    grad_b2 = delta_2.sum(axis=0)\n",
    "\n",
    "    delta_1 = delta_2 @ W2.T * Z1 * (1 - Z1)\n",
    "    grad_W1 = X.T @ delta_1 + alpha_1 * W1\n",
    "    grad_b1 = delta_1.sum(axis=0)\n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "\n",
    "\n",
    "def mini_batch(x_sample, y_sample, start_batch_size):\n",
    "    \"\"\"\n",
    "    Takes a copy of x_sample and y_sample and returns mini batch matrices of both and number of batches\n",
    "    \"\"\"\n",
    "\n",
    "    # Batches must divide evenly into total number of samples for numpy arrays\n",
    "    # get number of bathes by finding next smallest number that evenly divides\n",
    "    num_batches = start_batch_size\n",
    "    while len(x_sample) % num_batches != 0:\n",
    "        num_batches -= 1\n",
    "\n",
    "    # randomly shuffle indices\n",
    "    random_indices = np.random.choice(range(len(x_sample)), len(x_sample), replace=False)\n",
    "\n",
    "    # instantiate lists to hold batches\n",
    "    x_list = [[] for i in range(num_batches)]\n",
    "    y_list = [[] for i in range(num_batches)]\n",
    "\n",
    "    # populate batches matrix with random mini batch indices\n",
    "    for i in range(len(x_sample)):\n",
    "\n",
    "        x_list[i // 105].append(x_sample[random_indices[i]])\n",
    "        y_list[i // 105].append(y_sample[random_indices[i]])\n",
    "\n",
    "    x_batch = np.array(x_list)\n",
    "    y_batch = np.array(y_list)\n",
    "\n",
    "    return x_batch, y_batch, num_batches, num_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Gradient Descent\n",
    "\n",
    "For our first exercise we will impletment plain old gradient descet. The mathmatical formula is:\n",
    "\n",
    "$$ \\theta_t = \\theta_{t-1} - \\alpha \\nabla f(\\theta_{t-1}) \\tag{1}$$\n",
    "---\n",
    "\n",
    "We have already specified initial values for the $\\alpha$ but feel free to play around with it. Also note that we are taking mini batches of the entire sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vanilla Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-4\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "vanilla_loss = []\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "\n",
    "        \"\"\"\n",
    "        You put your code here. W1 and W2 are your weight matrices and are numpy arrays.\n",
    "        b1 and b2 are your bias for each matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        W1 -= eta * grad_W1\n",
    "        W2 -= eta * grad_W2\n",
    "        b1 -= eta * grad_b1\n",
    "        b2 -= eta * grad_b2\n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    vanilla_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum Gradient Descent\n",
    "\n",
    "Next we will add momentum. The mathmatical formula is:\n",
    "\n",
    "$$m_{t}=\\beta_{1} m_{t-1} + (1 - \\beta_{1})\\nabla f(\\theta_{t-1})$$\n",
    "---\n",
    "\n",
    "$$ \\beta_1 \\in [0,1)$$\n",
    "---\n",
    "\n",
    "$$\\theta_{t}=\\theta_{t-1} - \\alpha m_{t} \\tag{2}$$\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Momentum Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-4\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\"\"\"\n",
    "You need to add another hyper parameter here. Hint! look at the equation above\n",
    "\"\"\"\n",
    "Beta_1 = 0.9\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "momentum_loss = []\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "\n",
    "        \"\"\"\n",
    "        These are your gradients with respect to weight matrices W1 and W2 \n",
    "        as well as your biases b1 and b2\n",
    "        \"\"\"\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "\n",
    "        \"\"\"\n",
    "        You put your code here. W1 and W2 are your weight matrices,.\n",
    "        b1 and b2 are your bias for each matrix.\n",
    "        Each are numpy arrays.\n",
    "        \"\"\"\n",
    "\n",
    "        W1_m = W1 * Beta_1 + (1 - Beta_1) * grad_W1\n",
    "        W2_m = W2 * Beta_1 + (1 - Beta_1) * grad_W2\n",
    "        b1_m = b1 * Beta_1 + (1 - Beta_1) * grad_b1\n",
    "        b2_m = b2 * Beta_1 + (1 - Beta_1) * grad_b2\n",
    "        W1 -= eta * W1_m\n",
    "        W2 -= eta * W2_m\n",
    "        b1 -= eta * b1_m\n",
    "        b2 -= eta * b2_m\n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    momentum_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp Gradient Descent \n",
    "\n",
    "$$v_{t} = \\beta_{2}v_{t-1} + (1 - \\beta_{2})\\nabla f(\\theta_{t-1})^2$$\n",
    "---\n",
    "\n",
    "$$ \\beta_2 \\in [0,1)$$\n",
    "---\n",
    "\n",
    "$$\\theta_{t} = \\theta_{t-1} - \\alpha \\frac { \\nabla f(\\theta_{t-1})}{\\sqrt{v_{t} + \\epsilon}} \\tag{3}$$ \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RMSProp Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-4\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\"\"\"\n",
    "You need to add two hyper parameters here. Hint! look at the equation above\n",
    "\"\"\"\n",
    "Beta_2 = 0.9\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "RMS_loss = []\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "\n",
    "        \"\"\"\n",
    "        These are your gradients with respect to weight matrices W1 and W2 \n",
    "        as well as your biases b1 and b2\n",
    "        \"\"\"\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "\n",
    "        \"\"\"\n",
    "        You put your code here. W1 and W2 are your weight matrices,.\n",
    "        b1 and b2 are your bias for each matrix.\n",
    "        Each are numpy arrays.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        W1_v = W1 * Beta_2 + (1 - Beta_2) * grad_W1 * grad_W1\n",
    "        W2_v = W2 * Beta_2 + (1 - Beta_2) * grad_W2 * grad_W2\n",
    "        b1_v = b1 * Beta_2 + (1 - Beta_2) * grad_b1 * grad_b1\n",
    "        b2_v = b2 * Beta_2 + (1 - Beta_2) * grad_b2 * grad_b2\n",
    "        W1 -= eta * grad_W1 / np.sqrt(W1_v + epsilon)\n",
    "        W2 -= eta * grad_W2 / np.sqrt(W2_v + epsilon)\n",
    "        b1 -= eta * grad_b1 / np.sqrt(b1_v + epsilon)\n",
    "        b2 -= eta * grad_b2 / np.sqrt(b2_v + epsilon)\n",
    "\n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    RMS_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Gradient Descent \n",
    "\n",
    "$$m_{t}=\\beta_{1} m_{t-1} + (1 - \\beta_{1})\\nabla f(\\theta_{t-1})$$\n",
    "---\n",
    "\n",
    "$$v_{t} = \\beta_{2}v_{t-1} + (1 - \\beta_{2})\\nabla f(\\theta_{t-1})^2$$\n",
    "---\n",
    "\n",
    "$$\\hat m_{t} = \\frac {m_{t}}{1 - \\beta_1^{t}}$$\n",
    "---\n",
    "\n",
    "$$\\hat v_t = \\frac {v_{t}}{1 - \\beta_2^{t}}$$\n",
    "---\n",
    "\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\frac {\\hat m_t}{\\sqrt{\\hat v_t + \\epsilon}} \\tag{4}$$\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adam Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-4\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\"\"\"\n",
    "You need to add 3 hyper parameters here. Hint! look at the equation above\n",
    "\"\"\"\n",
    "Beta_1 = 0.9\n",
    "Beta_2 = 0.9\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "Adam_loss = []\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "\n",
    "        \"\"\"\n",
    "        These are your gradients with respect to weight matrices W1 and W2 \n",
    "        as well as your biases b1 and b2\n",
    "        \"\"\"\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "\n",
    "        \"\"\"\n",
    "        You put your code here. W1 and W2 are your weight matrices,.\n",
    "        b1 and b2 are your bias for each matrix.\n",
    "        Each are numpy arrays.\n",
    "        \"\"\"\n",
    "\n",
    "        W1_m = W1 * Beta_1 + (1 - Beta_1) * grad_W1\n",
    "        W2_m = W2 * Beta_1 + (1 - Beta_1) * grad_W2\n",
    "        b1_m = b1 * Beta_1 + (1 - Beta_1) * grad_b1\n",
    "        b2_m = b2 * Beta_1 + (1 - Beta_1) * grad_b2\n",
    "        W1_v = W1 * Beta_2 + (1 - Beta_2) * grad_W1 * grad_W1\n",
    "        W2_v = W2 * Beta_2 + (1 - Beta_2) * grad_W2 * grad_W2\n",
    "        b1_v = b1 * Beta_2 + (1 - Beta_2) * grad_b1 * grad_b1\n",
    "        b2_v = b2 * Beta_2 + (1 - Beta_2) * grad_b2 * grad_b2\n",
    "\n",
    "        W1_m_hat = W1_m / (1 - np.power(Beta_1, i))\n",
    "        W2_m_hat = W2_m / (1 - np.power(Beta_1, i))\n",
    "        b1_m_hat = b1_m / (1 - np.power(Beta_1, i))\n",
    "        b2_m_hat = b2_m / (1 - np.power(Beta_1, i))\n",
    "\n",
    "        W1_v_hat = W1_v / (1 - np.power(Beta_2, i))\n",
    "        W2_v_hat = W2_v / (1 - np.power(Beta_2, i))\n",
    "        b1_v_hat = b1_v / (1 - np.power(Beta_2, i))\n",
    "        b2_v_hat = b2_v / (1 - np.power(Beta_2, i))\n",
    "\n",
    "        W1 -= eta * W1_m_hat / np.sqrt(W1_v_hat + epsilon)\n",
    "        W2 -= eta * W2_m_hat / np.sqrt(W2_v_hat + epsilon)\n",
    "        b1 -= eta * b1_m_hat / np.sqrt(b1_v_hat + epsilon)\n",
    "        b2 -= eta * b2_m_hat / np.sqrt(b2_v_hat + epsilon)\n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    Adam_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
